{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.optim import Adam\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class inspired by https://github.com/emschenn/mlg-hw1/blob/master/DrBC.ipynb\n",
    "class Graph():\n",
    "    def __init__(self, batch_size: int, min_node: int, max_node: int) -> None:\n",
    "        \"\"\"Generate Graph by using networkx function.\"\"\"\n",
    "        self.graph_list = []\n",
    "        for i in range(batch_size):\n",
    "            g = nx.powerlaw_cluster_graph(n=random.randint(min_node,max_node), m=4, p=0.05)\n",
    "            self.graph_list.append(g)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the number of generated graph\"\"\"\n",
    "        return len(self.graph_list)\n",
    "    \n",
    "    def get_edge_index(self):\n",
    "        \"\"\"Get edge index.\"\"\"\n",
    "        src_list, tgt_list, base = [], [], 0\n",
    "        for g in self.graph_list:\n",
    "            for src, tgt in g.edges():\n",
    "                src_list.append(src+base)\n",
    "                tgt_list.append(tgt+base)\n",
    "            base += g.number_of_nodes()\n",
    "        edge_index = [src_list+tgt_list, tgt_list+src_list] # Combining src_list & tgt_list because undirected graph.\n",
    "\n",
    "        return torch.tensor(edge_index, dtype=torch.long).cuda()\n",
    "\n",
    "    def get_deg_list(self):\n",
    "        \"\"\"Get degree list.\"\"\"\n",
    "        deg_list = []\n",
    "        for g in self.graph_list:\n",
    "            num_nodes = g.number_of_nodes()\n",
    "            for i in range(num_nodes):\n",
    "                deg_list.append([g.degree[i], 1, 1])\n",
    "\n",
    "        return torch.Tensor(deg_list).cuda()\n",
    "    \n",
    "    def get_BC_values(self):\n",
    "        \"\"\"Get BC values.\"\"\"\n",
    "        BC_value = []\n",
    "        for g in self.graph_list:         \n",
    "            BC_value += list(nx.betweenness_centrality(g).values())\n",
    "        return torch.Tensor(BC_value).cuda()\n",
    "    \n",
    "    def get_pair_index(self):\n",
    "        \"\"\"Get pair index for doing pairwise ranking loss.\"\"\"\n",
    "        list_1, list_2, pair_index = [], [], []\n",
    "        for g in self.graph_list:\n",
    "            for i in range(g.number_of_nodes()):\n",
    "                list_1 += [i, i, i, i, i]\n",
    "                list_2 += [i, i, i, i, i]\n",
    "            random.shuffle(list_1)\n",
    "            random.shuffle(list_2)\n",
    "            for i, j in zip(list_1, list_2):\n",
    "                pair_index.append([i, j])\n",
    "            # initialize list1 & list2\n",
    "            list_1, list_2 = [], []\n",
    "            \n",
    "        return torch.tensor(pair_index, dtype=torch.long).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class inspired by https://github.com/emschenn/mlg-hw1/blob/master/DrBC.ipynb\n",
    "class read_test_data():\n",
    "    def __init__(self, data: str='youtube', file_num: int=0, testing: bool=False) -> None:\n",
    "        \"\"\"Setting test file path\"\"\"\n",
    "        if data == 'youtube':\n",
    "            data_path = './hw1_data/youtube/com-youtube.txt'\n",
    "            data_bc_value = './hw1_data/youtube/com-youtube_score.txt'\n",
    "        elif data == 'synthetic':\n",
    "            data_path = './hw1_data/Synthetic/5000/' + str(file_num) + '.txt'\n",
    "            data_bc_value = './hw1_data/Synthetic/5000/' + str(file_num) + '_score.txt'\n",
    "        else:\n",
    "            print('Error!! No such data name. Please try again. \\nCurrent data below:')\n",
    "            print(' - \\'youtube\\'\\n - \\'synthetic\\' (0-29 file_number)')\n",
    "            return\n",
    "        self.testing = testing\n",
    "        \"\"\"Opening test file\"\"\"\n",
    "        f_graph = open(data_path, mode='r')\n",
    "        f_bc_value = open(data_bc_value, mode='r')\n",
    "\n",
    "        \"\"\"Reading data\"\"\"\n",
    "        self.bc_value, src_list, tgt_list, self.deg_list, ind = [], [], [], [], 0\n",
    "        for line in f_bc_value:\n",
    "            _, val = line.split()\n",
    "            val = float(val)\n",
    "            self.bc_value.append([ind, val])\n",
    "            ind += 1\n",
    "        num_bc_value = len(self.bc_value)\n",
    "        for i in range(num_bc_value):\n",
    "            # initialize\n",
    "            self.deg_list.append([0, 1, 1])\n",
    "        for line in f_graph:\n",
    "            src, tgt = line.split()\n",
    "            src, tgt = int(src), int(tgt)\n",
    "            src_list.append(src)\n",
    "            tgt_list.append(tgt)\n",
    "            self.deg_list[src][0] += 1\n",
    "            self.deg_list[tgt][0] += 1\n",
    "        self.edge_index=[src_list+tgt_list, tgt_list+src_list]\n",
    "\n",
    "        f_graph.close()\n",
    "        f_bc_value.close()\n",
    "    \n",
    "    def get_edge_index(self):\n",
    "        if self.testing:\n",
    "            return torch.tensor(self.edge_index, dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.edge_index, dtype=torch.long).cuda()\n",
    "\n",
    "    def get_deg_list(self):\n",
    "        if self.testing:\n",
    "            return torch.Tensor(self.deg_list)\n",
    "        else:\n",
    "            return torch.Tensor(self.deg_list).cuda()\n",
    "    \n",
    "    def get_BC_values(self):\n",
    "        return self.bc_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBC(nn.Module):\n",
    "    \"\"\"DrBC model\n",
    "    They use 5 layers, all embedding dimension are set to be 128.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super(DrBC, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.norm1 = F.normalize\n",
    "\n",
    "        self.gcn1 = GCNConv(128, 128)\n",
    "        self.gru1 = nn.GRU(128, 128)\n",
    "        self.norm2 = F.normalize\n",
    "\n",
    "        self.gcn2 = GCNConv(128, 128)\n",
    "        self.gru2 = nn.GRU(128, 128)\n",
    "        self.norm3 = F.normalize\n",
    "\n",
    "        self.gcn3 = GCNConv(128, 128)\n",
    "        self.gru3 = nn.GRU(128, 128)\n",
    "        self.norm4 = F.normalize\n",
    "\n",
    "        self.gcn4 = GCNConv(128, 128)\n",
    "        self.gru4 = nn.GRU(128, 128)\n",
    "        self.norm5 = F.normalize\n",
    "\n",
    "        self.gcn5 = GCNConv(128, 128)\n",
    "        self.gru5 = nn.GRU(128, 128)\n",
    "        self.norm6 = F.normalize\n",
    "\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Encoder\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.norm1(x, p=2, dim=-1)\n",
    "\n",
    "        x_gcn = self.gcn1(x, edge_index)\n",
    "        x1, _ = self.gru1(x_gcn.view(1, *x_gcn.shape), x.view(1, *x.shape))\n",
    "        x1 = self.norm2(x1, p=2, dim=-1)\n",
    "\n",
    "        x_gcn = self.gcn2(x1[0], edge_index)\n",
    "        x2, _ = self.gru2(x_gcn.view(1, *x_gcn.shape), x1)\n",
    "        x2 = self.norm3(x2, p=2, dim=-1)\n",
    "\n",
    "        x_gcn = self.gcn3(x2[0], edge_index)\n",
    "        x3, _ = self.gru3(x_gcn.view(1, *x_gcn.shape), x2)\n",
    "        x3 = self.norm4(x3, p=2, dim=-1)\n",
    "\n",
    "        x_gcn = self.gcn4(x3[0], edge_index)\n",
    "        x4, _ = self.gru4(x_gcn.view(1, *x_gcn.shape), x3)\n",
    "        x4 = self.norm5(x4, p=2, dim=-1)\n",
    "\n",
    "        x_gcn = self.gcn5(x4[0], edge_index)\n",
    "        x5, _ = self.gru5(x_gcn.view(1, *x_gcn.shape), x4)\n",
    "        x5 = self.norm6(x5, p=2, dim=-1)\n",
    "\n",
    "        # maxpooling\n",
    "        l = [x1[0], x2[0], x3[0], x4[0], x5[0]]\n",
    "        l = torch.stack(l)\n",
    "        x = torch.max(l, dim = 0).values\n",
    "\n",
    "        # Decoder        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        output = self.fc3(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(batch_size: int, \n",
    "                   min_node: int, \n",
    "                   max_node: int, \n",
    "                   num_epoch: int=16, \n",
    "                   learning_rate: float=0.0001):\n",
    "    \n",
    "    \"\"\"set hyper-parameters.\"\"\"\n",
    "    max_iteration = 1120\n",
    "    iteration = int(max_iteration / batch_size) # 625 (if batch_size=16 & max_iteration=10000)\n",
    "    # num_epoch = int(max_iteration / iteration)  # 16  (if batch_size=16 & max_iteration=10000)\n",
    "    \"\"\"set training set & validation set.\"\"\"\n",
    "    total_train_graph_list = []\n",
    "    val_g = Graph(100, min_node, max_node)\n",
    "    \"\"\"set early-stopping.\"\"\"\n",
    "    last_loss = 999999\n",
    "    patience = 3\n",
    "    trigger_times = 0\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project = \"mlg_hw1\",\n",
    "        # set a run name (otherwise it'll randomly assigned)\n",
    "        name = str(min_node)+\"_\"+str(max_node)+\"_experiments\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config = {\n",
    "            \"architecture\": \"DrBC\",\n",
    "            \"dataset\": \"Synthetic\",\n",
    "            \"epochs\": num_epoch,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"iteration\":iteration,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"min_node\": min_node,\n",
    "            \"max_node\": max_node,\n",
    "            })\n",
    "    config = wandb.config\n",
    "\n",
    "    \"\"\"train & validate.\"\"\"\n",
    "    model = DrBC()\n",
    "    model = model.cuda()\n",
    "    optimizer = Adam(params=model.parameters(), lr=config.learning_rate)\n",
    "    for epoch in tqdm(range(1, config.epochs+1)):\n",
    "        # --- Train --------------------------------------------\n",
    "        model.train()\n",
    "        for iter in range(1, iteration+1):\n",
    "            optimizer.zero_grad() # set gradient to zero\n",
    "            if (iter-1)%5 == 0:\n",
    "                prev_time = time.time()\n",
    "            if epoch == 1:\n",
    "                g = Graph(config.batch_size, min_node, max_node)\n",
    "                total_train_graph_list.append(g)\n",
    "            g = total_train_graph_list[iter-1]\n",
    "            bc = g.get_BC_values()\n",
    "            outs = model(g.get_deg_list(), g.get_edge_index())\n",
    "            pair = g.get_pair_index()\n",
    "            pred = outs[pair[:, 0]] - outs[pair[:, 1]]\n",
    "\n",
    "            gt = torch.sigmoid(bc[pair[:, 0]] - bc[pair[:, 1]])\n",
    "            gt = gt.view(-1, 1)\n",
    "            tr_loss = F.binary_cross_entropy_with_logits(pred, gt, reduction='sum')\n",
    "            tr_loss.backward()\n",
    "            optimizer.step() # update model with optimizer\n",
    "            \n",
    "            # Show progress\n",
    "            if iter % 5 == 0 or iter == iteration:\n",
    "                print('[{}/{} Epoch, {}/{} Iter] Train_loss: {:.8} time:'.format(epoch, num_epoch, iter, iteration, tr_loss.item()),(time.time()-prev_time),\"sec\")\n",
    "            train_metrics = {\"train_loss\": tr_loss,\n",
    "                             \"train_epoch\": epoch-1}\n",
    "            # log train metrics to wandb\n",
    "            if iter + 1 < iteration: \n",
    "                \"\"\"Use this condition to prevent logging two times train_metrics \n",
    "                   when it logs the validation_metrics(write below).\n",
    "                \"\"\"\n",
    "                wandb.log(train_metrics)\n",
    "            \n",
    "        # --- Validate --------------------------------------------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_bc = val_g.get_BC_values()\n",
    "            val_outs = model(val_g.get_deg_list(), val_g.get_edge_index())\n",
    "            val_pair = val_g.get_pair_index()\n",
    "            val_pred = val_outs[val_pair[:, 0]] - val_outs[val_pair[:, 1]]\n",
    "\n",
    "            val_gt = torch.sigmoid(val_bc[val_pair[:, 0]] - val_bc[val_pair[:, 1]])\n",
    "            val_gt = val_gt.view(-1, 1)\n",
    "            val_loss = F.binary_cross_entropy_with_logits(val_pred, val_gt, reduction='sum')\n",
    "\n",
    "            # Early-stopping\n",
    "            cur_val_loss = val_loss\n",
    "            if epoch == 1:\n",
    "                last_val_loss = cur_val_loss\n",
    "                val_metrics = {\"val_loss\": last_val_loss, \n",
    "                               \"val_epoch\": epoch-1,\n",
    "                               \"earlystop_trigger\": trigger_times}\n",
    "                wandb.log({**train_metrics, **val_metrics})\n",
    "                print(\"Epoch[{}/{}] Val_Loss:{:.4f}\".format(epoch, num_epoch, cur_val_loss.item()), '  trigger times:', trigger_times)\n",
    "                continue\n",
    "            if cur_val_loss >= last_val_loss:\n",
    "                trigger_times += 1\n",
    "                if trigger_times >= patience:\n",
    "                    print(\"Epoch[{}/{}] Val_Loss:{:.4f}\".format(epoch, num_epoch, cur_val_loss.item()), '  trigger times:', trigger_times)\n",
    "                    print('Early Stopping!!\\nStart to test process.')\n",
    "                    # torch.save(model.state_dict(), \"./weight.pth\")\n",
    "                    return model\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "            last_val_loss = cur_val_loss\n",
    "\n",
    "        validation_metrics = {\"val_loss\": last_val_loss,\n",
    "                              \"val_epoch\": epoch-1,\n",
    "                              \"earlystop_trigger\": trigger_times}\n",
    "        # log train & validate metrics to wandb\n",
    "        wandb.log({**train_metrics, **validation_metrics})\n",
    "        print(\"Epoch[{}/{}] Val_Loss:{:.4f}\".format(epoch, num_epoch, last_val_loss.item()), '  trigger times:', trigger_times)\n",
    "    # torch.save(model.state_dict(), \"./weight.pth\")\n",
    "    wandb.finish()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def TopN_accuracy(outs, gt_data, n: int):\n",
    "    pred_bc_val, gt_bc_val = [], []\n",
    "    for i, j in enumerate(outs.tolist()):\n",
    "        pred_bc_val.append([i, *j])\n",
    "    gt_bc_val = gt_data.get_BC_values()\n",
    "\n",
    "    tmp_gt_bcval = gt_bc_val.copy()\n",
    "    tmp_pred_bcval =pred_bc_val.copy()\n",
    "    tmp_gt_bcval.sort(key=lambda element: element[1], reverse=True)\n",
    "    tmp_pred_bcval.sort(key=lambda element: element[1], reverse=True)\n",
    "    \n",
    "    pred, gt = [], []\n",
    "    for i in range(int(len(tmp_pred_bcval)*n/100)):\n",
    "        pred.append(tmp_pred_bcval[i][0])\n",
    "        gt.append(tmp_gt_bcval[i][0])\n",
    "    \n",
    "    return float(len(set(gt)&set(pred)) / len(pred))\n",
    "\n",
    "def Kendall_tau_distance(outs, gt_data):\n",
    "    pred_bc_val, gt_bc_val = [], []\n",
    "    for i, j in enumerate(outs.tolist()):\n",
    "        pred_bc_val.append(*j)\n",
    "    for i in gt_data.get_BC_values():\n",
    "        gt_bc_val.append(i[1])\n",
    "    tau, _ = stats.kendalltau(pred_bc_val, gt_bc_val)\n",
    "    \n",
    "    return float(tau)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    with torch.no_grad():\n",
    "        top1_list = []\n",
    "        top5_list = []\n",
    "        top10_list = []\n",
    "        kendall_ist = []\n",
    "        wall_clock_time_list = []\n",
    "\n",
    "        for data in test_data:\n",
    "            test_deg = data.get_deg_list()\n",
    "            test_edge_ind = data.get_edge_index()\n",
    "            t0 = time.time()\n",
    "            outs = model(test_deg, test_edge_ind)\n",
    "            wall_clock_time = time.time() - t0\n",
    "\n",
    "            top1_list.append(TopN_accuracy(outs, data, 1))\n",
    "            top5_list.append(TopN_accuracy(outs, data, 5))\n",
    "            top10_list.append(TopN_accuracy(outs, data, 10))\n",
    "            kendall_ist.append(Kendall_tau_distance(outs, data))\n",
    "            wall_clock_time_list.append(wall_clock_time)\n",
    "\n",
    "    # show result\n",
    "    if len(test_data) == 1:\n",
    "        print(\"Wall clock time:\", sum(wall_clock_time_list) / len(wall_clock_time_list))\n",
    "        print(\"top-1%:\", sum(top1_list) / len(top1_list))\n",
    "        print(\"top-5%:\", sum(top5_list) / len(top5_list))\n",
    "        print(\"top-10%:\", sum(top10_list) / len(top10_list))\n",
    "        print(\"kendall tau disance:\",sum(kendall_ist) / len(kendall_ist),\"\\n\")\n",
    "    else:\n",
    "        print(\"Wall clock time:\", sum(wall_clock_time_list) / len(wall_clock_time_list),'+-',statistics.stdev(wall_clock_time_list))\n",
    "        print(\"top-1%:\", sum(top1_list) / len(top1_list),'+-',statistics.stdev(top1_list))\n",
    "        print(\"top-5%:\", sum(top5_list) / len(top5_list),'+-',statistics.stdev(top5_list))\n",
    "        print(\"top-10%:\", sum(top10_list) / len(top10_list),'+-',statistics.stdev(top10_list))\n",
    "        print(\"kendall tau disance:\",sum(kendall_ist) / len(kendall_ist),'+-',statistics.stdev(kendall_ist),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "min_node = 100\n",
    "max_node = 200\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 1)\n",
    "torch.save(model, './models/testing_model.pt')\n",
    "model = torch.load('./models/testing_model.pt')\n",
    "\n",
    "# 'synthetic' test data\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)\n",
    "\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：100-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me19886893\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\lin-chia\\mlg_hw\\hw1\\wandb\\run-20230324_003134-ebtatzx6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e19886893/mlg_hw1/runs/ebtatzx6' target=\"_blank\">100_200_experiments</a></strong> to <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e19886893/mlg_hw1/runs/ebtatzx6' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/ebtatzx6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a61632afb54a7da204231761ad39a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/16 Epoch, 5/35 Iter] Train_loss: 16724.232 time: 11.325495481491089 sec\n",
      "[1/16 Epoch, 10/35 Iter] Train_loss: 16629.717 time: 11.66914415359497 sec\n",
      "[1/16 Epoch, 15/35 Iter] Train_loss: 17381.773 time: 11.528548955917358 sec\n",
      "[1/16 Epoch, 20/35 Iter] Train_loss: 16559.875 time: 11.606628656387329 sec\n",
      "[1/16 Epoch, 25/35 Iter] Train_loss: 16847.801 time: 11.216149806976318 sec\n",
      "[1/16 Epoch, 30/35 Iter] Train_loss: 16473.137 time: 11.122397422790527 sec\n",
      "[1/16 Epoch, 35/35 Iter] Train_loss: 17886.832 time: 11.341087102890015 sec\n",
      "Epoch[1/16] Val_Loss:53036.2578   trigger times: 0\n",
      "[2/16 Epoch, 5/35 Iter] Train_loss: 16722.451 time: 11.059908151626587 sec\n",
      "[2/16 Epoch, 10/35 Iter] Train_loss: 16628.512 time: 11.184850454330444 sec\n",
      "[2/16 Epoch, 15/35 Iter] Train_loss: 17381.047 time: 11.262988090515137 sec\n",
      "[2/16 Epoch, 20/35 Iter] Train_loss: 16559.408 time: 11.559818029403687 sec\n",
      "[2/16 Epoch, 25/35 Iter] Train_loss: 16847.502 time: 11.10674524307251 sec\n",
      "[2/16 Epoch, 30/35 Iter] Train_loss: 16472.77 time: 10.8099524974823 sec\n",
      "[2/16 Epoch, 35/35 Iter] Train_loss: 17886.312 time: 10.888078212738037 sec\n",
      "Epoch[2/16] Val_Loss:53035.9062   trigger times: 0\n",
      "[3/16 Epoch, 5/35 Iter] Train_loss: 16722.152 time: 11.138011693954468 sec\n",
      "[3/16 Epoch, 10/35 Iter] Train_loss: 16628.293 time: 11.356687545776367 sec\n",
      "[3/16 Epoch, 15/35 Iter] Train_loss: 17380.988 time: 11.153637409210205 sec\n",
      "[3/16 Epoch, 20/35 Iter] Train_loss: 16559.355 time: 11.466063976287842 sec\n",
      "[3/16 Epoch, 25/35 Iter] Train_loss: 16847.447 time: 11.075559377670288 sec\n",
      "[3/16 Epoch, 30/35 Iter] Train_loss: 16472.627 time: 10.950559139251709 sec\n",
      "[3/16 Epoch, 35/35 Iter] Train_loss: 17886.119 time: 11.137986660003662 sec\n",
      "Epoch[3/16] Val_Loss:53035.6875   trigger times: 0\n",
      "[4/16 Epoch, 5/35 Iter] Train_loss: 16722.07 time: 11.138044357299805 sec\n",
      "[4/16 Epoch, 10/35 Iter] Train_loss: 16628.18 time: 11.403578281402588 sec\n",
      "[4/16 Epoch, 15/35 Iter] Train_loss: 17380.979 time: 11.247363567352295 sec\n",
      "[4/16 Epoch, 20/35 Iter] Train_loss: 16559.355 time: 11.294228553771973 sec\n",
      "[4/16 Epoch, 25/35 Iter] Train_loss: 16847.438 time: 11.106780290603638 sec\n",
      "[4/16 Epoch, 30/35 Iter] Train_loss: 16472.652 time: 11.028632640838623 sec\n",
      "[4/16 Epoch, 35/35 Iter] Train_loss: 17886.109 time: 11.044297218322754 sec\n",
      "Epoch[4/16] Val_Loss:53035.6641   trigger times: 0\n",
      "[5/16 Epoch, 5/35 Iter] Train_loss: 16722.059 time: 10.99745225906372 sec\n",
      "[5/16 Epoch, 10/35 Iter] Train_loss: 16628.084 time: 11.466040134429932 sec\n",
      "[5/16 Epoch, 15/35 Iter] Train_loss: 17380.963 time: 11.356738328933716 sec\n",
      "[5/16 Epoch, 20/35 Iter] Train_loss: 16559.314 time: 11.356685400009155 sec\n",
      "[5/16 Epoch, 25/35 Iter] Train_loss: 16847.387 time: 11.138015031814575 sec\n",
      "[5/16 Epoch, 30/35 Iter] Train_loss: 16472.598 time: 10.950587749481201 sec\n",
      "[5/16 Epoch, 35/35 Iter] Train_loss: 17886.035 time: 11.028673648834229 sec\n",
      "Epoch[5/16] Val_Loss:53035.7344   trigger times: 1\n",
      "[6/16 Epoch, 5/35 Iter] Train_loss: 16722.018 time: 10.934942483901978 sec\n",
      "[6/16 Epoch, 10/35 Iter] Train_loss: 16628.057 time: 11.309852123260498 sec\n",
      "[6/16 Epoch, 15/35 Iter] Train_loss: 17380.977 time: 11.262953758239746 sec\n",
      "[6/16 Epoch, 20/35 Iter] Train_loss: 16559.287 time: 11.41922640800476 sec\n",
      "[6/16 Epoch, 25/35 Iter] Train_loss: 16847.381 time: 11.091151475906372 sec\n",
      "[6/16 Epoch, 30/35 Iter] Train_loss: 16472.609 time: 11.013045072555542 sec\n",
      "[6/16 Epoch, 35/35 Iter] Train_loss: 17886.057 time: 10.950589418411255 sec\n",
      "Epoch[6/16] Val_Loss:53035.7109   trigger times: 0\n",
      "[7/16 Epoch, 5/35 Iter] Train_loss: 16722.031 time: 11.153663873672485 sec\n",
      "[7/16 Epoch, 10/35 Iter] Train_loss: 16628.078 time: 11.434821367263794 sec\n",
      "[7/16 Epoch, 15/35 Iter] Train_loss: 17380.986 time: 11.294201374053955 sec\n",
      "[7/16 Epoch, 20/35 Iter] Train_loss: 16559.324 time: 11.231770992279053 sec\n",
      "[7/16 Epoch, 25/35 Iter] Train_loss: 16847.385 time: 11.059884071350098 sec\n",
      "[7/16 Epoch, 30/35 Iter] Train_loss: 16472.609 time: 10.919341802597046 sec\n",
      "[7/16 Epoch, 35/35 Iter] Train_loss: 17886.08 time: 11.137989044189453 sec\n",
      "Epoch[7/16] Val_Loss:53035.7109   trigger times: 0\n",
      "[8/16 Epoch, 5/35 Iter] Train_loss: 16722.084 time: 10.903696537017822 sec\n",
      "[8/16 Epoch, 10/35 Iter] Train_loss: 16628.074 time: 11.481658935546875 sec\n",
      "[8/16 Epoch, 15/35 Iter] Train_loss: 17380.938 time: 11.356721878051758 sec\n",
      "[8/16 Epoch, 20/35 Iter] Train_loss: 16559.322 time: 11.481706619262695 sec\n",
      "[8/16 Epoch, 25/35 Iter] Train_loss: 16847.357 time: 10.981801986694336 sec\n",
      "[8/16 Epoch, 30/35 Iter] Train_loss: 16472.602 time: 11.013046026229858 sec\n",
      "[8/16 Epoch, 35/35 Iter] Train_loss: 17886.062 time: 10.981802463531494 sec\n",
      "Epoch[8/16] Val_Loss:53035.7812   trigger times: 1\n",
      "[9/16 Epoch, 5/35 Iter] Train_loss: 16722.016 time: 10.997425079345703 sec\n",
      "[9/16 Epoch, 10/35 Iter] Train_loss: 16628.043 time: 11.294224500656128 sec\n",
      "[9/16 Epoch, 15/35 Iter] Train_loss: 17381.0 time: 11.24734091758728 sec\n",
      "[9/16 Epoch, 20/35 Iter] Train_loss: 16559.34 time: 11.512958526611328 sec\n",
      "[9/16 Epoch, 25/35 Iter] Train_loss: 16847.348 time: 10.903677701950073 sec\n",
      "[9/16 Epoch, 30/35 Iter] Train_loss: 16472.584 time: 11.044301509857178 sec\n",
      "[9/16 Epoch, 35/35 Iter] Train_loss: 17886.072 time: 11.013046979904175 sec\n",
      "Epoch[9/16] Val_Loss:53035.8047   trigger times: 2\n",
      "[10/16 Epoch, 5/35 Iter] Train_loss: 16721.994 time: 10.93493914604187 sec\n",
      "[10/16 Epoch, 10/35 Iter] Train_loss: 16628.068 time: 11.309849739074707 sec\n",
      "[10/16 Epoch, 15/35 Iter] Train_loss: 17380.949 time: 11.278608560562134 sec\n",
      "[10/16 Epoch, 20/35 Iter] Train_loss: 16559.312 time: 11.403578996658325 sec\n",
      "[10/16 Epoch, 25/35 Iter] Train_loss: 16847.361 time: 11.075536012649536 sec\n",
      "[10/16 Epoch, 30/35 Iter] Train_loss: 16472.629 time: 11.013043880462646 sec\n",
      "[10/16 Epoch, 35/35 Iter] Train_loss: 17886.037 time: 11.059902429580688 sec\n",
      "Epoch[10/16] Val_Loss:53035.7812   trigger times: 0\n",
      "[11/16 Epoch, 5/35 Iter] Train_loss: 16721.953 time: 10.997394323348999 sec\n",
      "[11/16 Epoch, 10/35 Iter] Train_loss: 16628.074 time: 11.419228553771973 sec\n",
      "[11/16 Epoch, 15/35 Iter] Train_loss: 17380.928 time: 11.325443029403687 sec\n",
      "[11/16 Epoch, 20/35 Iter] Train_loss: 16559.324 time: 11.419199705123901 sec\n",
      "[11/16 Epoch, 25/35 Iter] Train_loss: 16847.416 time: 11.247364521026611 sec\n",
      "[11/16 Epoch, 30/35 Iter] Train_loss: 16472.621 time: 10.825615882873535 sec\n",
      "[11/16 Epoch, 35/35 Iter] Train_loss: 17886.051 time: 11.138018369674683 sec\n",
      "Epoch[11/16] Val_Loss:53035.9688   trigger times: 1\n",
      "[12/16 Epoch, 5/35 Iter] Train_loss: 16722.051 time: 11.153664350509644 sec\n",
      "[12/16 Epoch, 10/35 Iter] Train_loss: 16628.037 time: 11.21612286567688 sec\n",
      "[12/16 Epoch, 15/35 Iter] Train_loss: 17380.945 time: 11.262958526611328 sec\n",
      "[12/16 Epoch, 20/35 Iter] Train_loss: 16559.311 time: 11.341125726699829 sec\n",
      "[12/16 Epoch, 25/35 Iter] Train_loss: 16847.379 time: 11.059904098510742 sec\n",
      "[12/16 Epoch, 30/35 Iter] Train_loss: 16472.604 time: 10.950567245483398 sec\n",
      "[12/16 Epoch, 35/35 Iter] Train_loss: 17886.059 time: 11.10676622390747 sec\n",
      "Epoch[12/16] Val_Loss:53035.8750   trigger times: 0\n",
      "[13/16 Epoch, 5/35 Iter] Train_loss: 16721.984 time: 11.091151475906372 sec\n",
      "[13/16 Epoch, 10/35 Iter] Train_loss: 16627.979 time: 11.231772899627686 sec\n",
      "[13/16 Epoch, 15/35 Iter] Train_loss: 17380.957 time: 11.294229507446289 sec\n",
      "[13/16 Epoch, 20/35 Iter] Train_loss: 16559.336 time: 11.341122388839722 sec\n",
      "[13/16 Epoch, 25/35 Iter] Train_loss: 16847.363 time: 11.138015031814575 sec\n",
      "[13/16 Epoch, 30/35 Iter] Train_loss: 16472.617 time: 10.888073205947876 sec\n",
      "[13/16 Epoch, 35/35 Iter] Train_loss: 17886.053 time: 11.02863883972168 sec\n",
      "Epoch[13/16] Val_Loss:53035.8594   trigger times: 0\n",
      "[14/16 Epoch, 5/35 Iter] Train_loss: 16722.037 time: 11.013044834136963 sec\n",
      "[14/16 Epoch, 10/35 Iter] Train_loss: 16628.068 time: 11.434793949127197 sec\n",
      "[14/16 Epoch, 15/35 Iter] Train_loss: 17380.953 time: 11.278607606887817 sec\n",
      "[14/16 Epoch, 20/35 Iter] Train_loss: 16559.324 time: 11.466063976287842 sec\n",
      "[14/16 Epoch, 25/35 Iter] Train_loss: 16847.395 time: 11.24736475944519 sec\n",
      "[14/16 Epoch, 30/35 Iter] Train_loss: 16472.629 time: 10.778756141662598 sec\n",
      "[14/16 Epoch, 35/35 Iter] Train_loss: 17886.08 time: 11.091147422790527 sec\n",
      "Epoch[14/16] Val_Loss:53035.9531   trigger times: 1\n",
      "[15/16 Epoch, 5/35 Iter] Train_loss: 16721.996 time: 11.137988567352295 sec\n",
      "[15/16 Epoch, 10/35 Iter] Train_loss: 16628.004 time: 11.309849977493286 sec\n",
      "[15/16 Epoch, 15/35 Iter] Train_loss: 17380.938 time: 11.18490719795227 sec\n",
      "[15/16 Epoch, 20/35 Iter] Train_loss: 16559.303 time: 11.434818744659424 sec\n",
      "[15/16 Epoch, 25/35 Iter] Train_loss: 16847.363 time: 11.106777906417847 sec\n",
      "[15/16 Epoch, 30/35 Iter] Train_loss: 16472.602 time: 10.934937000274658 sec\n",
      "[15/16 Epoch, 35/35 Iter] Train_loss: 17886.07 time: 11.10677170753479 sec\n",
      "Epoch[15/16] Val_Loss:53036.0000   trigger times: 2\n",
      "[16/16 Epoch, 5/35 Iter] Train_loss: 16722.016 time: 11.091177940368652 sec\n",
      "[16/16 Epoch, 10/35 Iter] Train_loss: 16628.061 time: 11.309848308563232 sec\n",
      "[16/16 Epoch, 15/35 Iter] Train_loss: 17380.902 time: 11.262991428375244 sec\n",
      "[16/16 Epoch, 20/35 Iter] Train_loss: 16559.312 time: 11.32546877861023 sec\n",
      "[16/16 Epoch, 25/35 Iter] Train_loss: 16847.346 time: 10.981802940368652 sec\n",
      "[16/16 Epoch, 30/35 Iter] Train_loss: 16472.68 time: 10.841182470321655 sec\n",
      "[16/16 Epoch, 35/35 Iter] Train_loss: 17886.098 time: 11.153645515441895 sec\n",
      "Epoch[16/16] Val_Loss:53035.8359   trigger times: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4cded814f647358f5bb5ba8e087a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.075402…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>▁▁▁▁▅▁▁▅█▁▅▁▁▅█▁</td></tr><tr><td>train_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>▄▁▁▆▆▆▄▄▄▄▄▁▁▆▃▆▄▄▄▄▄▄▁▆▃▆▄▄▆▄▄▄▁▆▃▃▄▄▆█</td></tr><tr><td>val_epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▄▁▁▂▂▂▂▃▂▅▃▃▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>0</td></tr><tr><td>train_epoch</td><td>15</td></tr><tr><td>train_loss</td><td>17886.09766</td></tr><tr><td>val_epoch</td><td>15</td></tr><tr><td>val_loss</td><td>53035.83594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">100_200_experiments</strong> at: <a href='https://wandb.ai/e19886893/mlg_hw1/runs/ebtatzx6' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/ebtatzx6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_003134-ebtatzx6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 100\n",
    "max_node = 200\n",
    "lr = 0.00008\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in synthetic data:\n",
      "Wall clock time: 0.13851188818613688 +- 0.012798659712158317\n",
      "top-1%: 0.9033333333333338 +- 0.029749915483314692\n",
      "top-5%: 0.8412000000000001 +- 0.02074874321079679\n",
      "top-10%: 0.8084000000000002 +- 0.01731971171218223\n",
      "kendall tau disance: 0.4443821200059742 +- 0.012122740177771667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 100\n",
    "max_node = 200\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in youtube data:\n",
      "Wall clock time: 29.149409532546997\n",
      "top-1%: 0.6817060274938315\n",
      "top-5%: 0.6162942337515861\n",
      "top-10%: 0.5763025491457322\n",
      "kendall tau disance: 0.10276323958690384 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 100\n",
    "max_node = 200\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：200-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me19886893\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\lin-chia\\mlg_hw\\hw1\\wandb\\run-20230324_010942-do9ok1fq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e19886893/mlg_hw1/runs/do9ok1fq' target=\"_blank\">200_300_experiments</a></strong> to <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e19886893/mlg_hw1/runs/do9ok1fq' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/do9ok1fq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3c2293567f4296b00e63a7df0d4267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/16 Epoch, 5/35 Iter] Train_loss: 28067.969 time: 30.258564710617065 sec\n",
      "[1/16 Epoch, 10/35 Iter] Train_loss: 27588.439 time: 28.790122270584106 sec\n",
      "[1/16 Epoch, 15/35 Iter] Train_loss: 28478.238 time: 30.03984570503235 sec\n",
      "[1/16 Epoch, 20/35 Iter] Train_loss: 27023.113 time: 29.149421453475952 sec\n",
      "[1/16 Epoch, 25/35 Iter] Train_loss: 26971.342 time: 27.77471351623535 sec\n",
      "[1/16 Epoch, 30/35 Iter] Train_loss: 27718.695 time: 29.274418592453003 sec\n",
      "[1/16 Epoch, 35/35 Iter] Train_loss: 27652.219 time: 28.946316242218018 sec\n",
      "Epoch[1/16] Val_Loss:86555.8125   trigger times: 0\n",
      "[2/16 Epoch, 5/35 Iter] Train_loss: 28065.834 time: 29.78988027572632 sec\n",
      "[2/16 Epoch, 10/35 Iter] Train_loss: 27587.512 time: 28.524581909179688 sec\n",
      "[2/16 Epoch, 15/35 Iter] Train_loss: 28477.52 time: 29.680545568466187 sec\n",
      "[2/16 Epoch, 20/35 Iter] Train_loss: 27022.553 time: 28.587053775787354 sec\n",
      "[2/16 Epoch, 25/35 Iter] Train_loss: 26970.533 time: 27.72788119316101 sec\n",
      "[2/16 Epoch, 30/35 Iter] Train_loss: 27718.045 time: 28.72764277458191 sec\n",
      "[2/16 Epoch, 35/35 Iter] Train_loss: 27651.352 time: 28.69640612602234 sec\n",
      "Epoch[2/16] Val_Loss:86554.5234   trigger times: 0\n",
      "[3/16 Epoch, 5/35 Iter] Train_loss: 28065.541 time: 29.77423858642578 sec\n",
      "[3/16 Epoch, 10/35 Iter] Train_loss: 27587.281 time: 28.352760076522827 sec\n",
      "[3/16 Epoch, 15/35 Iter] Train_loss: 28477.359 time: 29.7274112701416 sec\n",
      "[3/16 Epoch, 20/35 Iter] Train_loss: 27022.457 time: 28.727638483047485 sec\n",
      "[3/16 Epoch, 25/35 Iter] Train_loss: 26970.33 time: 27.74350666999817 sec\n",
      "[3/16 Epoch, 30/35 Iter] Train_loss: 27717.906 time: 28.93072199821472 sec\n",
      "[3/16 Epoch, 35/35 Iter] Train_loss: 27651.104 time: 28.696375370025635 sec\n",
      "Epoch[3/16] Val_Loss:86554.3438   trigger times: 0\n",
      "[4/16 Epoch, 5/35 Iter] Train_loss: 28065.523 time: 29.774274349212646 sec\n",
      "[4/16 Epoch, 10/35 Iter] Train_loss: 27587.207 time: 28.36835479736328 sec\n",
      "[4/16 Epoch, 15/35 Iter] Train_loss: 28477.322 time: 29.727410316467285 sec\n",
      "[4/16 Epoch, 20/35 Iter] Train_loss: 27022.428 time: 28.727644681930542 sec\n",
      "[4/16 Epoch, 25/35 Iter] Train_loss: 26970.273 time: 27.930978059768677 sec\n",
      "[4/16 Epoch, 30/35 Iter] Train_loss: 27717.986 time: 28.540167808532715 sec\n",
      "[4/16 Epoch, 35/35 Iter] Train_loss: 27651.047 time: 28.665159702301025 sec\n",
      "Epoch[4/16] Val_Loss:86554.2969   trigger times: 0\n",
      "[5/16 Epoch, 5/35 Iter] Train_loss: 28065.51 time: 29.821139574050903 sec\n",
      "[5/16 Epoch, 10/35 Iter] Train_loss: 27587.186 time: 28.337110996246338 sec\n",
      "[5/16 Epoch, 15/35 Iter] Train_loss: 28477.338 time: 29.89927625656128 sec\n",
      "[5/16 Epoch, 20/35 Iter] Train_loss: 27022.445 time: 28.758856773376465 sec\n",
      "[5/16 Epoch, 25/35 Iter] Train_loss: 26970.307 time: 27.743502855300903 sec\n",
      "[5/16 Epoch, 30/35 Iter] Train_loss: 27717.955 time: 28.743263959884644 sec\n",
      "[5/16 Epoch, 35/35 Iter] Train_loss: 27651.057 time: 28.790158987045288 sec\n",
      "Epoch[5/16] Val_Loss:86554.3750   trigger times: 1\n",
      "[6/16 Epoch, 5/35 Iter] Train_loss: 28065.504 time: 29.96175980567932 sec\n",
      "[6/16 Epoch, 10/35 Iter] Train_loss: 27587.172 time: 28.33708667755127 sec\n",
      "[6/16 Epoch, 15/35 Iter] Train_loss: 28477.34 time: 29.696192502975464 sec\n",
      "[6/16 Epoch, 20/35 Iter] Train_loss: 27022.41 time: 28.75885844230652 sec\n",
      "[6/16 Epoch, 25/35 Iter] Train_loss: 26970.348 time: 27.743500232696533 sec\n",
      "[6/16 Epoch, 30/35 Iter] Train_loss: 27717.945 time: 28.94637441635132 sec\n",
      "[6/16 Epoch, 35/35 Iter] Train_loss: 27650.949 time: 28.71199321746826 sec\n",
      "Epoch[6/16] Val_Loss:86554.3281   trigger times: 0\n",
      "[7/16 Epoch, 5/35 Iter] Train_loss: 28065.514 time: 29.774248600006104 sec\n",
      "[7/16 Epoch, 10/35 Iter] Train_loss: 27587.188 time: 28.321518421173096 sec\n",
      "[7/16 Epoch, 15/35 Iter] Train_loss: 28477.297 time: 29.696168899536133 sec\n",
      "[7/16 Epoch, 20/35 Iter] Train_loss: 27022.445 time: 28.961935997009277 sec\n",
      "[7/16 Epoch, 25/35 Iter] Train_loss: 26970.35 time: 27.743500471115112 sec\n",
      "[7/16 Epoch, 30/35 Iter] Train_loss: 27717.926 time: 28.540217638015747 sec\n",
      "[7/16 Epoch, 35/35 Iter] Train_loss: 27650.943 time: 28.508946895599365 sec\n",
      "Epoch[7/16] Val_Loss:86554.3281   trigger times: 0\n",
      "[8/16 Epoch, 5/35 Iter] Train_loss: 28065.512 time: 29.977351903915405 sec\n",
      "[8/16 Epoch, 10/35 Iter] Train_loss: 27587.182 time: 28.33708381652832 sec\n",
      "[8/16 Epoch, 15/35 Iter] Train_loss: 28477.316 time: 29.930516004562378 sec\n",
      "[8/16 Epoch, 20/35 Iter] Train_loss: 27022.465 time: 28.680752754211426 sec\n",
      "[8/16 Epoch, 25/35 Iter] Train_loss: 26970.324 time: 27.79039192199707 sec\n",
      "[8/16 Epoch, 30/35 Iter] Train_loss: 27717.959 time: 28.743239402770996 sec\n",
      "[8/16 Epoch, 35/35 Iter] Train_loss: 27650.883 time: 28.72767686843872 sec\n",
      "Epoch[8/16] Val_Loss:86554.2344   trigger times: 0\n",
      "[9/16 Epoch, 5/35 Iter] Train_loss: 28065.492 time: 29.977351903915405 sec\n",
      "[9/16 Epoch, 10/35 Iter] Train_loss: 27587.16 time: 28.5089750289917 sec\n",
      "[9/16 Epoch, 15/35 Iter] Train_loss: 28477.316 time: 29.743030071258545 sec\n",
      "[9/16 Epoch, 20/35 Iter] Train_loss: 27022.455 time: 28.727645874023438 sec\n",
      "[9/16 Epoch, 25/35 Iter] Train_loss: 26970.32 time: 27.743472576141357 sec\n",
      "[9/16 Epoch, 30/35 Iter] Train_loss: 27717.955 time: 28.93072271347046 sec\n",
      "[9/16 Epoch, 35/35 Iter] Train_loss: 27650.951 time: 28.72767448425293 sec\n",
      "Epoch[9/16] Val_Loss:86554.2969   trigger times: 1\n",
      "[10/16 Epoch, 5/35 Iter] Train_loss: 28065.449 time: 29.774243354797363 sec\n",
      "[10/16 Epoch, 10/35 Iter] Train_loss: 27587.135 time: 28.337111949920654 sec\n",
      "[10/16 Epoch, 15/35 Iter] Train_loss: 28477.309 time: 29.71182155609131 sec\n",
      "[10/16 Epoch, 20/35 Iter] Train_loss: 27022.449 time: 28.727612733840942 sec\n",
      "[10/16 Epoch, 25/35 Iter] Train_loss: 26970.365 time: 27.74350094795227 sec\n",
      "[10/16 Epoch, 30/35 Iter] Train_loss: 27717.951 time: 28.915128707885742 sec\n",
      "[10/16 Epoch, 35/35 Iter] Train_loss: 27650.898 time: 28.696398496627808 sec\n",
      "Epoch[10/16] Val_Loss:86554.3047   trigger times: 2\n",
      "[11/16 Epoch, 5/35 Iter] Train_loss: 28065.469 time: 29.977380752563477 sec\n",
      "[11/16 Epoch, 10/35 Iter] Train_loss: 27587.148 time: 28.493324518203735 sec\n",
      "[11/16 Epoch, 15/35 Iter] Train_loss: 28477.303 time: 29.571168422698975 sec\n",
      "[11/16 Epoch, 20/35 Iter] Train_loss: 27022.461 time: 28.727674961090088 sec\n",
      "[11/16 Epoch, 25/35 Iter] Train_loss: 26970.344 time: 27.946547746658325 sec\n",
      "[11/16 Epoch, 30/35 Iter] Train_loss: 27717.918 time: 28.72764539718628 sec\n",
      "[11/16 Epoch, 35/35 Iter] Train_loss: 27650.926 time: 28.524567365646362 sec\n",
      "Epoch[11/16] Val_Loss:86554.2500   trigger times: 0\n",
      "[12/16 Epoch, 5/35 Iter] Train_loss: 28065.486 time: 29.774275541305542 sec\n",
      "[12/16 Epoch, 10/35 Iter] Train_loss: 27587.168 time: 28.305869340896606 sec\n",
      "[12/16 Epoch, 15/35 Iter] Train_loss: 28477.295 time: 29.96170210838318 sec\n",
      "[12/16 Epoch, 20/35 Iter] Train_loss: 27022.43 time: 28.72767472267151 sec\n",
      "[12/16 Epoch, 25/35 Iter] Train_loss: 26970.395 time: 27.743470430374146 sec\n",
      "[12/16 Epoch, 30/35 Iter] Train_loss: 27717.965 time: 28.88387155532837 sec\n",
      "[12/16 Epoch, 35/35 Iter] Train_loss: 27650.859 time: 28.774506092071533 sec\n",
      "Epoch[12/16] Val_Loss:86554.2031   trigger times: 0\n",
      "[13/16 Epoch, 5/35 Iter] Train_loss: 28065.477 time: 29.961758852005005 sec\n",
      "[13/16 Epoch, 10/35 Iter] Train_loss: 27587.158 time: 28.337084531784058 sec\n",
      "[13/16 Epoch, 15/35 Iter] Train_loss: 28477.316 time: 29.74306035041809 sec\n",
      "[13/16 Epoch, 20/35 Iter] Train_loss: 27022.426 time: 28.72764492034912 sec\n",
      "[13/16 Epoch, 25/35 Iter] Train_loss: 26970.395 time: 27.72784996032715 sec\n",
      "[13/16 Epoch, 30/35 Iter] Train_loss: 27717.918 time: 28.930750131607056 sec\n",
      "[13/16 Epoch, 35/35 Iter] Train_loss: 27650.854 time: 28.696375131607056 sec\n",
      "Epoch[13/16] Val_Loss:86554.2812   trigger times: 1\n",
      "[14/16 Epoch, 5/35 Iter] Train_loss: 28065.469 time: 29.977325201034546 sec\n",
      "[14/16 Epoch, 10/35 Iter] Train_loss: 27587.131 time: 28.321516752243042 sec\n",
      "[14/16 Epoch, 15/35 Iter] Train_loss: 28477.334 time: 29.696167945861816 sec\n",
      "[14/16 Epoch, 20/35 Iter] Train_loss: 27022.488 time: 28.74325966835022 sec\n",
      "[14/16 Epoch, 25/35 Iter] Train_loss: 26970.375 time: 27.743480443954468 sec\n",
      "[14/16 Epoch, 30/35 Iter] Train_loss: 27717.914 time: 28.55583691596985 sec\n",
      "[14/16 Epoch, 35/35 Iter] Train_loss: 27650.871 time: 28.508946895599365 sec\n",
      "Epoch[14/16] Val_Loss:86554.2656   trigger times: 0\n",
      "[15/16 Epoch, 5/35 Iter] Train_loss: 28065.455 time: 29.789868354797363 sec\n",
      "[15/16 Epoch, 10/35 Iter] Train_loss: 27587.162 time: 28.290274381637573 sec\n",
      "[15/16 Epoch, 15/35 Iter] Train_loss: 28477.262 time: 29.961704969406128 sec\n",
      "[15/16 Epoch, 20/35 Iter] Train_loss: 27022.43 time: 28.696431398391724 sec\n",
      "[15/16 Epoch, 25/35 Iter] Train_loss: 26970.338 time: 27.77471351623535 sec\n",
      "[15/16 Epoch, 30/35 Iter] Train_loss: 27717.934 time: 28.555837392807007 sec\n",
      "[15/16 Epoch, 35/35 Iter] Train_loss: 27650.904 time: 28.66513156890869 sec\n",
      "Epoch[15/16] Val_Loss:86554.2344   trigger times: 0\n",
      "[16/16 Epoch, 5/35 Iter] Train_loss: 28065.441 time: 29.97737979888916 sec\n",
      "[16/16 Epoch, 10/35 Iter] Train_loss: 27587.123 time: 28.52457022666931 sec\n",
      "[16/16 Epoch, 15/35 Iter] Train_loss: 28477.318 time: 29.74302911758423 sec\n",
      "[16/16 Epoch, 20/35 Iter] Train_loss: 27022.422 time: 28.727617502212524 sec\n",
      "[16/16 Epoch, 25/35 Iter] Train_loss: 26970.334 time: 27.727879524230957 sec\n",
      "[16/16 Epoch, 30/35 Iter] Train_loss: 27717.984 time: 28.930749654769897 sec\n",
      "[16/16 Epoch, 35/35 Iter] Train_loss: 27650.871 time: 28.602640628814697 sec\n",
      "Epoch[16/16] Val_Loss:86554.2656   trigger times: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6dfcf894a241419e6153047e09e1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>▁▁▁▁▅▁▁▁▅█▁▁▅▁▁▅</td></tr><tr><td>train_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>▅▁▂▃▃▇▁█▃▄▅▁▂▃▃▇▁█▃▄▅▁▂▃▃▇▁█▄▄▅▁▂▃▃█▁█▄▃</td></tr><tr><td>val_epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>1</td></tr><tr><td>train_epoch</td><td>15</td></tr><tr><td>train_loss</td><td>27650.87109</td></tr><tr><td>val_epoch</td><td>15</td></tr><tr><td>val_loss</td><td>86554.26562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">200_300_experiments</strong> at: <a href='https://wandb.ai/e19886893/mlg_hw1/runs/do9ok1fq' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/do9ok1fq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_010942-do9ok1fq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 200\n",
    "max_node = 300\n",
    "lr = 0.00008\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in synthetic data:\n",
      "Wall clock time: 0.1385127305984497 +- 0.015745694808447227\n",
      "top-1%: 0.9140000000000005 +- 0.02736723437326794\n",
      "top-5%: 0.8588 +- 0.019935067006451843\n",
      "top-10%: 0.8289999999999998 +- 0.018418506435385697\n",
      "kendall tau disance: 0.4978663197460111 +- 0.01780422318074113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 200\n",
    "max_node = 300\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in youtube data:\n",
      "Wall clock time: 29.477470874786377\n",
      "top-1%: 0.6691046880507578\n",
      "top-5%: 0.4978147469335965\n",
      "top-10%: 0.35267735199006073\n",
      "kendall tau disance: -0.2657820805851588 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py:5278: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 200\n",
    "max_node = 300\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：1000-1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\lin-chia\\mlg_hw\\hw1\\wandb\\run-20230324_021526-wnvckqt0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e19886893/mlg_hw1/runs/wnvckqt0' target=\"_blank\">1000_1200_experiments</a></strong> to <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e19886893/mlg_hw1/runs/wnvckqt0' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/wnvckqt0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb6c1b44e54e91ad444dc2ef92798b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/16 Epoch, 5/35 Iter] Train_loss: 124479.59 time: 562.4151163101196 sec\n",
      "[1/16 Epoch, 10/35 Iter] Train_loss: 123505.82 time: 558.1661126613617 sec\n",
      "[1/16 Epoch, 15/35 Iter] Train_loss: 120209.86 time: 542.7634718418121 sec\n",
      "[1/16 Epoch, 20/35 Iter] Train_loss: 121952.95 time: 554.7762820720673 sec\n",
      "[1/16 Epoch, 25/35 Iter] Train_loss: 122057.77 time: 564.6646175384521 sec\n",
      "[1/16 Epoch, 30/35 Iter] Train_loss: 123145.45 time: 556.1821825504303 sec\n",
      "[1/16 Epoch, 35/35 Iter] Train_loss: 121557.05 time: 558.8066141605377 sec\n",
      "Epoch[1/16] Val_Loss:379047.6875   trigger times: 0\n",
      "[2/16 Epoch, 5/35 Iter] Train_loss: 124478.27 time: 576.5680804252625 sec\n",
      "[2/16 Epoch, 10/35 Iter] Train_loss: 123505.09 time: 567.8669941425323 sec\n",
      "[2/16 Epoch, 15/35 Iter] Train_loss: 120209.56 time: 552.1206548213959 sec\n",
      "[2/16 Epoch, 20/35 Iter] Train_loss: 121952.64 time: 559.8375961780548 sec\n",
      "[2/16 Epoch, 25/35 Iter] Train_loss: 122057.66 time: 570.2882454395294 sec\n",
      "[2/16 Epoch, 30/35 Iter] Train_loss: 123145.41 time: 559.8220021724701 sec\n",
      "[2/16 Epoch, 35/35 Iter] Train_loss: 121556.98 time: 559.6188957691193 sec\n",
      "Epoch[2/16] Val_Loss:379046.7500   trigger times: 0\n",
      "[3/16 Epoch, 5/35 Iter] Train_loss: 124478.08 time: 575.8807415962219 sec\n",
      "[3/16 Epoch, 10/35 Iter] Train_loss: 123505.07 time: 570.7881572246552 sec\n",
      "[3/16 Epoch, 15/35 Iter] Train_loss: 120209.55 time: 552.6361556053162 sec\n",
      "[3/16 Epoch, 20/35 Iter] Train_loss: 121952.55 time: 560.2125120162964 sec\n",
      "[3/16 Epoch, 25/35 Iter] Train_loss: 122057.65 time: 569.4135096073151 sec\n",
      "[3/16 Epoch, 30/35 Iter] Train_loss: 123145.36 time: 559.0096621513367 sec\n",
      "[3/16 Epoch, 35/35 Iter] Train_loss: 121556.95 time: 560.3999652862549 sec\n",
      "Epoch[3/16] Val_Loss:379046.5000   trigger times: 0\n",
      "[4/16 Epoch, 5/35 Iter] Train_loss: 124478.05 time: 576.0994117259979 sec\n",
      "[4/16 Epoch, 10/35 Iter] Train_loss: 123505.11 time: 569.4603695869446 sec\n",
      "[4/16 Epoch, 15/35 Iter] Train_loss: 120209.52 time: 551.9644408226013 sec\n",
      "[4/16 Epoch, 20/35 Iter] Train_loss: 121952.52 time: 561.7902636528015 sec\n",
      "[4/16 Epoch, 25/35 Iter] Train_loss: 122057.64 time: 569.663449048996 sec\n",
      "[4/16 Epoch, 30/35 Iter] Train_loss: 123145.38 time: 558.462917804718 sec\n",
      "[4/16 Epoch, 35/35 Iter] Train_loss: 121556.95 time: 560.7904987335205 sec\n",
      "Epoch[4/16] Val_Loss:379046.4375   trigger times: 0\n",
      "[5/16 Epoch, 5/35 Iter] Train_loss: 124478.0 time: 576.0837905406952 sec\n",
      "[5/16 Epoch, 10/35 Iter] Train_loss: 123505.1 time: 569.7883908748627 sec\n",
      "[5/16 Epoch, 15/35 Iter] Train_loss: 120209.55 time: 551.9956538677216 sec\n",
      "[5/16 Epoch, 20/35 Iter] Train_loss: 121952.54 time: 561.7902634143829 sec\n",
      "[5/16 Epoch, 25/35 Iter] Train_loss: 122057.64 time: 570.1008462905884 sec\n",
      "[5/16 Epoch, 30/35 Iter] Train_loss: 123145.38 time: 558.8221797943115 sec\n",
      "[5/16 Epoch, 35/35 Iter] Train_loss: 121556.97 time: 560.2281296253204 sec\n",
      "Epoch[5/16] Val_Loss:379046.3438   trigger times: 0\n",
      "[6/16 Epoch, 5/35 Iter] Train_loss: 124477.95 time: 576.2244119644165 sec\n",
      "[6/16 Epoch, 10/35 Iter] Train_loss: 123505.07 time: 569.4759910106659 sec\n",
      "[6/16 Epoch, 15/35 Iter] Train_loss: 120209.51 time: 552.3393888473511 sec\n",
      "[6/16 Epoch, 20/35 Iter] Train_loss: 121952.52 time: 561.0404715538025 sec\n",
      "[6/16 Epoch, 25/35 Iter] Train_loss: 122057.64 time: 570.4288623332977 sec\n",
      "[6/16 Epoch, 30/35 Iter] Train_loss: 123145.36 time: 559.7907357215881 sec\n",
      "[6/16 Epoch, 35/35 Iter] Train_loss: 121556.95 time: 559.415846824646 sec\n",
      "Epoch[6/16] Val_Loss:379046.3125   trigger times: 0\n",
      "[7/16 Epoch, 5/35 Iter] Train_loss: 124477.94 time: 576.8179953098297 sec\n",
      "[7/16 Epoch, 10/35 Iter] Train_loss: 123505.07 time: 569.4759619235992 sec\n",
      "[7/16 Epoch, 15/35 Iter] Train_loss: 120209.51 time: 552.6205368041992 sec\n",
      "[7/16 Epoch, 20/35 Iter] Train_loss: 121952.5 time: 561.2122719287872 sec\n",
      "[7/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 569.2885096073151 sec\n",
      "[7/16 Epoch, 30/35 Iter] Train_loss: 123145.34 time: 559.9938094615936 sec\n",
      "[7/16 Epoch, 35/35 Iter] Train_loss: 121556.91 time: 559.9938094615936 sec\n",
      "Epoch[7/16] Val_Loss:379046.3125   trigger times: 0\n",
      "[8/16 Epoch, 5/35 Iter] Train_loss: 124477.98 time: 576.2712805271149 sec\n",
      "[8/16 Epoch, 10/35 Iter] Train_loss: 123505.11 time: 570.0383579730988 sec\n",
      "[8/16 Epoch, 15/35 Iter] Train_loss: 120209.49 time: 552.1831123828888 sec\n",
      "[8/16 Epoch, 20/35 Iter] Train_loss: 121952.52 time: 561.4153814315796 sec\n",
      "[8/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 569.5072057247162 sec\n",
      "[8/16 Epoch, 30/35 Iter] Train_loss: 123145.35 time: 559.4470641613007 sec\n",
      "[8/16 Epoch, 35/35 Iter] Train_loss: 121556.95 time: 559.5876529216766 sec\n",
      "Epoch[8/16] Val_Loss:379046.3125   trigger times: 0\n",
      "[9/16 Epoch, 5/35 Iter] Train_loss: 124477.95 time: 576.8492331504822 sec\n",
      "[9/16 Epoch, 10/35 Iter] Train_loss: 123505.1 time: 569.1010534763336 sec\n",
      "[9/16 Epoch, 15/35 Iter] Train_loss: 120209.5 time: 551.605149269104 sec\n",
      "[9/16 Epoch, 20/35 Iter] Train_loss: 121952.53 time: 562.0089628696442 sec\n",
      "[9/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 570.6319408416748 sec\n",
      "[9/16 Epoch, 30/35 Iter] Train_loss: 123145.33 time: 559.0721518993378 sec\n",
      "[9/16 Epoch, 35/35 Iter] Train_loss: 121556.92 time: 560.9154675006866 sec\n",
      "Epoch[9/16] Val_Loss:379046.2500   trigger times: 0\n",
      "[10/16 Epoch, 5/35 Iter] Train_loss: 124477.97 time: 584.3162360191345 sec\n",
      "[10/16 Epoch, 10/35 Iter] Train_loss: 123505.09 time: 570.2258162498474 sec\n",
      "[10/16 Epoch, 15/35 Iter] Train_loss: 120209.5 time: 552.1831662654877 sec\n",
      "[10/16 Epoch, 20/35 Iter] Train_loss: 121952.48 time: 562.633796453476 sec\n",
      "[10/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 570.6319603919983 sec\n",
      "[10/16 Epoch, 30/35 Iter] Train_loss: 123145.34 time: 558.9159371852875 sec\n",
      "[10/16 Epoch, 35/35 Iter] Train_loss: 121556.91 time: 560.6186640262604 sec\n",
      "Epoch[10/16] Val_Loss:379046.2500   trigger times: 0\n",
      "[11/16 Epoch, 5/35 Iter] Train_loss: 124477.94 time: 576.3337028026581 sec\n",
      "[11/16 Epoch, 10/35 Iter] Train_loss: 123505.08 time: 570.6475632190704 sec\n",
      "[11/16 Epoch, 15/35 Iter] Train_loss: 120209.5 time: 553.8077883720398 sec\n",
      "[11/16 Epoch, 20/35 Iter] Train_loss: 121952.48 time: 562.6181950569153 sec\n",
      "[11/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 571.2567965984344 sec\n",
      "[11/16 Epoch, 30/35 Iter] Train_loss: 123145.34 time: 565.6253747940063 sec\n",
      "[11/16 Epoch, 35/35 Iter] Train_loss: 121556.92 time: 560.8061227798462 sec\n",
      "Epoch[11/16] Val_Loss:379046.2188   trigger times: 0\n",
      "[12/16 Epoch, 5/35 Iter] Train_loss: 124477.95 time: 578.6457185745239 sec\n",
      "[12/16 Epoch, 10/35 Iter] Train_loss: 123505.06 time: 570.0227091312408 sec\n",
      "[12/16 Epoch, 15/35 Iter] Train_loss: 120209.47 time: 554.385749578476 sec\n",
      "[12/16 Epoch, 20/35 Iter] Train_loss: 121952.49 time: 563.477367401123 sec\n",
      "[12/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 572.0534846782684 sec\n",
      "[12/16 Epoch, 30/35 Iter] Train_loss: 123145.35 time: 560.1968593597412 sec\n",
      "[12/16 Epoch, 35/35 Iter] Train_loss: 121556.94 time: 561.8059122562408 sec\n",
      "Epoch[12/16] Val_Loss:379046.2500   trigger times: 1\n",
      "[13/16 Epoch, 5/35 Iter] Train_loss: 124477.92 time: 576.0525767803192 sec\n",
      "[13/16 Epoch, 10/35 Iter] Train_loss: 123505.05 time: 569.6321761608124 sec\n",
      "[13/16 Epoch, 15/35 Iter] Train_loss: 120209.47 time: 553.1360681056976 sec\n",
      "[13/16 Epoch, 20/35 Iter] Train_loss: 121952.47 time: 560.8529856204987 sec\n",
      "[13/16 Epoch, 25/35 Iter] Train_loss: 122057.61 time: 570.0539510250092 sec\n",
      "[13/16 Epoch, 30/35 Iter] Train_loss: 123145.33 time: 559.1970944404602 sec\n",
      "[13/16 Epoch, 35/35 Iter] Train_loss: 121556.92 time: 559.447062253952 sec\n",
      "Epoch[13/16] Val_Loss:379046.2500   trigger times: 0\n",
      "[14/16 Epoch, 5/35 Iter] Train_loss: 124477.92 time: 576.3805921077728 sec\n",
      "[14/16 Epoch, 10/35 Iter] Train_loss: 123505.05 time: 569.4447231292725 sec\n",
      "[14/16 Epoch, 15/35 Iter] Train_loss: 120209.48 time: 551.1989924907684 sec\n",
      "[14/16 Epoch, 20/35 Iter] Train_loss: 121952.48 time: 560.3999722003937 sec\n",
      "[14/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 569.444717168808 sec\n",
      "[14/16 Epoch, 30/35 Iter] Train_loss: 123145.34 time: 558.7909698486328 sec\n",
      "[14/16 Epoch, 35/35 Iter] Train_loss: 121556.91 time: 559.5407567024231 sec\n",
      "Epoch[14/16] Val_Loss:379046.2188   trigger times: 0\n",
      "[15/16 Epoch, 5/35 Iter] Train_loss: 124477.95 time: 575.88068318367 sec\n",
      "[15/16 Epoch, 10/35 Iter] Train_loss: 123505.06 time: 568.4449830055237 sec\n",
      "[15/16 Epoch, 15/35 Iter] Train_loss: 120209.48 time: 551.120890378952 sec\n",
      "[15/16 Epoch, 20/35 Iter] Train_loss: 121952.48 time: 561.0091922283173 sec\n",
      "[15/16 Epoch, 25/35 Iter] Train_loss: 122057.61 time: 569.8665018081665 sec\n",
      "[15/16 Epoch, 30/35 Iter] Train_loss: 123145.32 time: 558.2598106861115 sec\n",
      "[15/16 Epoch, 35/35 Iter] Train_loss: 121556.9 time: 559.7907648086548 sec\n",
      "Epoch[15/16] Val_Loss:379046.1875   trigger times: 0\n",
      "[16/16 Epoch, 5/35 Iter] Train_loss: 124477.94 time: 575.662041425705 sec\n",
      "[16/16 Epoch, 10/35 Iter] Train_loss: 123505.07 time: 568.8198404312134 sec\n",
      "[16/16 Epoch, 15/35 Iter] Train_loss: 120209.48 time: 551.5739114284515 sec\n",
      "[16/16 Epoch, 20/35 Iter] Train_loss: 121952.48 time: 562.030561208725 sec\n",
      "[16/16 Epoch, 25/35 Iter] Train_loss: 122057.62 time: 569.4447474479675 sec\n",
      "[16/16 Epoch, 30/35 Iter] Train_loss: 123145.34 time: 557.8068242073059 sec\n",
      "[16/16 Epoch, 35/35 Iter] Train_loss: 121556.91 time: 559.603277683258 sec\n",
      "Epoch[16/16] Val_Loss:379046.1875   trigger times: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0e7820bb604ad89dac3da6b17e24c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.075886…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>train_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>▇█▃▄█▆▅▆█▇▇█▃▄▇▆▅▆█▇▇▆▃▄▇▆▅▆▄▇▇▆▃▄▇▁▅▆▄▅</td></tr><tr><td>val_epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>earlystop_trigger</td><td>0</td></tr><tr><td>train_epoch</td><td>15</td></tr><tr><td>train_loss</td><td>121556.90625</td></tr><tr><td>val_epoch</td><td>15</td></tr><tr><td>val_loss</td><td>379046.1875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1000_1200_experiments</strong> at: <a href='https://wandb.ai/e19886893/mlg_hw1/runs/wnvckqt0' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/wnvckqt0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_021526-wnvckqt0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr = 0.00008\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in synthetic data:\n",
      "Wall clock time: 0.15204795201619467 +- 0.016881041557345253\n",
      "top-1%: 0.9286666666666671 +- 0.025014938065819396\n",
      "top-5%: 0.8790666666666669 +- 0.018515479699585096\n",
      "top-10%: 0.8595333333333335 +- 0.016845614047688456\n",
      "kendall tau disance: 0.6591245288626779 +- 0.016678040660441186 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in youtube data:\n",
      "Wall clock time: 28.883826732635498\n",
      "top-1%: 0.6913993655269651\n",
      "top-5%: 0.35154377555336247\n",
      "top-10%: 0.21886702676030276\n",
      "kendall tau disance: -0.3616086335279728 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr = 0.00008\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_00008model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：1000-1200, batch=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b24c17beea149a99d81d7344e48893d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/16 Epoch, 5/35 Iter] Train_loss: 122421.59 time: 547.371741771698 sec\n",
      "[1/16 Epoch, 10/35 Iter] Train_loss: 122494.53 time: 546.6688342094421 sec\n",
      "[1/16 Epoch, 15/35 Iter] Train_loss: 124340.7 time: 556.9007573127747 sec\n",
      "[1/16 Epoch, 20/35 Iter] Train_loss: 120275.1 time: 571.0693392753601 sec\n",
      "[1/16 Epoch, 25/35 Iter] Train_loss: 120486.72 time: 551.855122089386 sec\n",
      "[1/16 Epoch, 30/35 Iter] Train_loss: 122254.24 time: 559.7594583034515 sec\n",
      "[1/16 Epoch, 35/35 Iter] Train_loss: 119925.39 time: 560.7123913764954 sec\n",
      "Epoch[1/16] Val_Loss:379098.9375\n",
      "[2/16 Epoch, 5/35 Iter] Train_loss: 122420.73 time: 559.2283911705017 sec\n",
      "[2/16 Epoch, 10/35 Iter] Train_loss: 122493.97 time: 556.6820895671844 sec\n",
      "[2/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 562.6494407653809 sec\n",
      "[2/16 Epoch, 20/35 Iter] Train_loss: 120274.72 time: 562.8837523460388 sec\n",
      "[2/16 Epoch, 25/35 Iter] Train_loss: 120486.61 time: 556.2290372848511 sec\n",
      "[2/16 Epoch, 30/35 Iter] Train_loss: 122254.22 time: 562.6806797981262 sec\n",
      "[2/16 Epoch, 35/35 Iter] Train_loss: 119925.45 time: 561.446594953537 sec\n",
      "Epoch[2/16] Val_Loss:379098.8750\n",
      "[3/16 Epoch, 5/35 Iter] Train_loss: 122420.73 time: 559.5720324516296 sec\n",
      "[3/16 Epoch, 10/35 Iter] Train_loss: 122493.87 time: 556.1509304046631 sec\n",
      "[3/16 Epoch, 15/35 Iter] Train_loss: 124340.48 time: 561.8683700561523 sec\n",
      "[3/16 Epoch, 20/35 Iter] Train_loss: 120274.76 time: 562.8525149822235 sec\n",
      "[3/16 Epoch, 25/35 Iter] Train_loss: 120486.61 time: 555.2449328899384 sec\n",
      "[3/16 Epoch, 30/35 Iter] Train_loss: 122254.33 time: 562.4776196479797 sec\n",
      "[3/16 Epoch, 35/35 Iter] Train_loss: 119925.41 time: 561.196626663208 sec\n",
      "Epoch[3/16] Val_Loss:379098.9062\n",
      "[4/16 Epoch, 5/35 Iter] Train_loss: 122420.7 time: 559.572062253952 sec\n",
      "[4/16 Epoch, 10/35 Iter] Train_loss: 122493.85 time: 556.1196875572205 sec\n",
      "[4/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 562.2589042186737 sec\n",
      "[4/16 Epoch, 20/35 Iter] Train_loss: 120274.67 time: 563.2587015628815 sec\n",
      "[4/16 Epoch, 25/35 Iter] Train_loss: 120486.64 time: 555.1042983531952 sec\n",
      "[4/16 Epoch, 30/35 Iter] Train_loss: 122254.23 time: 563.102468252182 sec\n",
      "[4/16 Epoch, 35/35 Iter] Train_loss: 119925.38 time: 561.6809303760529 sec\n",
      "Epoch[4/16] Val_Loss:379098.7188\n",
      "[5/16 Epoch, 5/35 Iter] Train_loss: 122420.71 time: 559.8219375610352 sec\n",
      "[5/16 Epoch, 10/35 Iter] Train_loss: 122493.85 time: 557.9005780220032 sec\n",
      "[5/16 Epoch, 15/35 Iter] Train_loss: 124340.48 time: 562.4775750637054 sec\n",
      "[5/16 Epoch, 20/35 Iter] Train_loss: 120274.72 time: 563.4617471694946 sec\n",
      "[5/16 Epoch, 25/35 Iter] Train_loss: 120486.57 time: 555.7917010784149 sec\n",
      "[5/16 Epoch, 30/35 Iter] Train_loss: 122254.27 time: 562.7118911743164 sec\n",
      "[5/16 Epoch, 35/35 Iter] Train_loss: 119925.42 time: 561.2591381072998 sec\n",
      "Epoch[5/16] Val_Loss:379098.7812\n",
      "[6/16 Epoch, 5/35 Iter] Train_loss: 122420.71 time: 559.259605884552 sec\n",
      "[6/16 Epoch, 10/35 Iter] Train_loss: 122493.86 time: 556.7133295536041 sec\n",
      "[6/16 Epoch, 15/35 Iter] Train_loss: 124340.47 time: 562.8525140285492 sec\n",
      "[6/16 Epoch, 20/35 Iter] Train_loss: 120274.64 time: 563.086835861206 sec\n",
      "[6/16 Epoch, 25/35 Iter] Train_loss: 120486.63 time: 555.682318687439 sec\n",
      "[6/16 Epoch, 30/35 Iter] Train_loss: 122254.2 time: 563.0868351459503 sec\n",
      "[6/16 Epoch, 35/35 Iter] Train_loss: 119925.33 time: 562.0558578968048 sec\n",
      "Epoch[6/16] Val_Loss:379098.6875\n",
      "[7/16 Epoch, 5/35 Iter] Train_loss: 122420.69 time: 560.1812655925751 sec\n",
      "[7/16 Epoch, 10/35 Iter] Train_loss: 122493.86 time: 556.7446002960205 sec\n",
      "[7/16 Epoch, 15/35 Iter] Train_loss: 124340.47 time: 562.4463305473328 sec\n",
      "[7/16 Epoch, 20/35 Iter] Train_loss: 120274.72 time: 563.4773685932159 sec\n",
      "[7/16 Epoch, 25/35 Iter] Train_loss: 120486.58 time: 556.0572323799133 sec\n",
      "[7/16 Epoch, 30/35 Iter] Train_loss: 122254.22 time: 562.8837897777557 sec\n",
      "[7/16 Epoch, 35/35 Iter] Train_loss: 119925.36 time: 561.6808819770813 sec\n",
      "Epoch[7/16] Val_Loss:379098.6875\n",
      "[8/16 Epoch, 5/35 Iter] Train_loss: 122420.7 time: 559.8375680446625 sec\n",
      "[8/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 557.1038951873779 sec\n",
      "[8/16 Epoch, 15/35 Iter] Train_loss: 124340.47 time: 563.0555610656738 sec\n",
      "[8/16 Epoch, 20/35 Iter] Train_loss: 120274.68 time: 563.3055653572083 sec\n",
      "[8/16 Epoch, 25/35 Iter] Train_loss: 120486.54 time: 556.400869846344 sec\n",
      "[8/16 Epoch, 30/35 Iter] Train_loss: 122254.2 time: 563.4305047988892 sec\n",
      "[8/16 Epoch, 35/35 Iter] Train_loss: 119925.38 time: 562.118311882019 sec\n",
      "Epoch[8/16] Val_Loss:379098.6875\n",
      "[9/16 Epoch, 5/35 Iter] Train_loss: 122420.7 time: 559.9469401836395 sec\n",
      "[9/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 556.5727088451385 sec\n",
      "[9/16 Epoch, 15/35 Iter] Train_loss: 124340.48 time: 562.883786201477 sec\n",
      "[9/16 Epoch, 20/35 Iter] Train_loss: 120274.67 time: 563.4460959434509 sec\n",
      "[9/16 Epoch, 25/35 Iter] Train_loss: 120486.58 time: 556.2759304046631 sec\n",
      "[9/16 Epoch, 30/35 Iter] Train_loss: 122254.23 time: 563.4773991107941 sec\n",
      "[9/16 Epoch, 35/35 Iter] Train_loss: 119925.32 time: 561.2747628688812 sec\n",
      "Epoch[9/16] Val_Loss:379098.6875\n",
      "[10/16 Epoch, 5/35 Iter] Train_loss: 122420.69 time: 559.8063535690308 sec\n",
      "[10/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 557.119482755661 sec\n",
      "[10/16 Epoch, 15/35 Iter] Train_loss: 124340.47 time: 562.8837909698486 sec\n",
      "[10/16 Epoch, 20/35 Iter] Train_loss: 120274.67 time: 563.2273941040039 sec\n",
      "[10/16 Epoch, 25/35 Iter] Train_loss: 120486.55 time: 556.0728533267975 sec\n",
      "[10/16 Epoch, 30/35 Iter] Train_loss: 122254.22 time: 563.071213722229 sec\n",
      "[10/16 Epoch, 35/35 Iter] Train_loss: 119925.34 time: 562.461982011795 sec\n",
      "Epoch[10/16] Val_Loss:379098.6250\n",
      "[11/16 Epoch, 5/35 Iter] Train_loss: 122420.68 time: 560.196914434433 sec\n",
      "[11/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 557.5256109237671 sec\n",
      "[11/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 563.1180772781372 sec\n",
      "[11/16 Epoch, 20/35 Iter] Train_loss: 120274.66 time: 562.8681361675262 sec\n",
      "[11/16 Epoch, 25/35 Iter] Train_loss: 120486.57 time: 556.26034283638 sec\n",
      "[11/16 Epoch, 30/35 Iter] Train_loss: 122254.23 time: 562.8368604183197 sec\n",
      "[11/16 Epoch, 35/35 Iter] Train_loss: 119925.34 time: 561.837155342102 sec\n",
      "Epoch[11/16] Val_Loss:379098.6875\n",
      "[12/16 Epoch, 5/35 Iter] Train_loss: 122420.7 time: 561.2123029232025 sec\n",
      "[12/16 Epoch, 10/35 Iter] Train_loss: 122493.83 time: 556.510251045227 sec\n",
      "[12/16 Epoch, 15/35 Iter] Train_loss: 124340.46 time: 562.7119219303131 sec\n",
      "[12/16 Epoch, 20/35 Iter] Train_loss: 120274.66 time: 563.2742609977722 sec\n",
      "[12/16 Epoch, 25/35 Iter] Train_loss: 120486.58 time: 556.4477949142456 sec\n",
      "[12/16 Epoch, 30/35 Iter] Train_loss: 122254.22 time: 563.1492910385132 sec\n",
      "[12/16 Epoch, 35/35 Iter] Train_loss: 119925.31 time: 561.4622156620026 sec\n",
      "Epoch[12/16] Val_Loss:379098.6250\n",
      "[13/16 Epoch, 5/35 Iter] Train_loss: 122420.69 time: 560.6186306476593 sec\n",
      "[13/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 558.1505191326141 sec\n",
      "[13/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 565.8674066066742 sec\n",
      "[13/16 Epoch, 20/35 Iter] Train_loss: 120274.66 time: 563.2742912769318 sec\n",
      "[13/16 Epoch, 25/35 Iter] Train_loss: 120486.58 time: 557.4162883758545 sec\n",
      "[13/16 Epoch, 30/35 Iter] Train_loss: 122254.19 time: 580.6017167568207 sec\n",
      "[13/16 Epoch, 35/35 Iter] Train_loss: 119925.3 time: 587.7216866016388 sec\n",
      "Epoch[13/16] Val_Loss:379098.6250\n",
      "[14/16 Epoch, 5/35 Iter] Train_loss: 122420.7 time: 574.2873358726501 sec\n",
      "[14/16 Epoch, 10/35 Iter] Train_loss: 122493.83 time: 581.1763744354248 sec\n",
      "[14/16 Epoch, 15/35 Iter] Train_loss: 124340.44 time: 562.524439573288 sec\n",
      "[14/16 Epoch, 20/35 Iter] Train_loss: 120274.64 time: 563.4773685932159 sec\n",
      "[14/16 Epoch, 25/35 Iter] Train_loss: 120486.55 time: 559.0409080982208 sec\n",
      "[14/16 Epoch, 30/35 Iter] Train_loss: 122254.18 time: 563.3211553096771 sec\n",
      "[14/16 Epoch, 35/35 Iter] Train_loss: 119925.3 time: 561.4153521060944 sec\n",
      "Epoch[14/16] Val_Loss:379098.6250\n",
      "[15/16 Epoch, 5/35 Iter] Train_loss: 122420.69 time: 561.0404689311981 sec\n",
      "[15/16 Epoch, 10/35 Iter] Train_loss: 122493.84 time: 557.9630336761475 sec\n",
      "[15/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 562.8993513584137 sec\n",
      "[15/16 Epoch, 20/35 Iter] Train_loss: 120274.64 time: 563.4461257457733 sec\n",
      "[15/16 Epoch, 25/35 Iter] Train_loss: 120486.56 time: 555.8697757720947 sec\n",
      "[15/16 Epoch, 30/35 Iter] Train_loss: 122254.16 time: 565.1332581043243 sec\n",
      "[15/16 Epoch, 35/35 Iter] Train_loss: 119925.3 time: 561.49343085289 sec\n",
      "Epoch[15/16] Val_Loss:379098.5938\n",
      "[16/16 Epoch, 5/35 Iter] Train_loss: 122420.69 time: 559.7594885826111 sec\n",
      "[16/16 Epoch, 10/35 Iter] Train_loss: 122493.83 time: 556.369658946991 sec\n",
      "[16/16 Epoch, 15/35 Iter] Train_loss: 124340.45 time: 562.649439573288 sec\n",
      "[16/16 Epoch, 20/35 Iter] Train_loss: 120274.62 time: 562.6806509494781 sec\n",
      "[16/16 Epoch, 25/35 Iter] Train_loss: 120486.58 time: 556.0572597980499 sec\n",
      "[16/16 Epoch, 30/35 Iter] Train_loss: 122254.14 time: 562.4463593959808 sec\n",
      "[16/16 Epoch, 35/35 Iter] Train_loss: 119925.3 time: 561.0716826915741 sec\n",
      "Epoch[16/16] Val_Loss:379098.6250\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.0005\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in synthetic data:\n",
      "Wall clock time: 0.1359073797861735 +- 0.01240942738964881\n",
      "top-1%: 0.9280000000000006 +- 0.02605034912731826\n",
      "top-5%: 0.8784000000000003 +- 0.01790703964829245\n",
      "top-10%: 0.8614666666666667 +- 0.0174903585920404\n",
      "kendall tau disance: 0.6886484222564367 +- 0.01889704099299993 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.0005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test in youtube data:\n",
      "Wall clock time: 31.19578528404236\n",
      "top-1%: 0.6979203383856186\n",
      "top-5%: 0.3965000704920344\n",
      "top-10%: 0.2079584805575871\n",
      "kendall tau disance: -0.43096769409637614 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\stats\\_stats_py.py:5278: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.0005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：1000-1200, batch=32, lr=0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.00005\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.00005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i, testing=True)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "min_node = 1000\n",
    "max_node = 1200\n",
    "lr=0.00005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not done) experiments (cost too much time...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：2000-3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6ef52u5t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd7161c68c44525a651e0e883212ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.212857…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2000_3000_experiments</strong> at: <a href='https://wandb.ai/e19886893/mlg_hw1/runs/6ef52u5t' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/6ef52u5t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230323_212947-6ef52u5t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6ef52u5t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd16680fdda54f22b2a91756fcc1825e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\lin-chia\\mlg_hw\\hw1\\wandb\\run-20230323_215111-ul0vubq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e19886893/mlg_hw1/runs/ul0vubq6' target=\"_blank\">2000_3000_experiments</a></strong> to <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e19886893/mlg_hw1' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e19886893/mlg_hw1/runs/ul0vubq6' target=\"_blank\">https://wandb.ai/e19886893/mlg_hw1/runs/ul0vubq6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cc1f53f17347ef9a515a7bf645ed1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m max_node \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m\n\u001b[0;32m      4\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.00005\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m train_validate(batch_size, min_node, max_node, \u001b[39m16\u001b[39;49m, lr)\n\u001b[0;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39m'\u001b[39m\u001b[39m./models/_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(batch_size)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(min_node)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(max_node)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_model.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 52\u001b[0m, in \u001b[0;36mtrain_validate\u001b[1;34m(batch_size, min_node, max_node, num_epoch, learning_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m     total_train_graph_list\u001b[39m.\u001b[39mappend(g)\n\u001b[0;32m     51\u001b[0m g \u001b[39m=\u001b[39m total_train_graph_list[\u001b[39miter\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 52\u001b[0m bc \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49mget_BC_values()\n\u001b[0;32m     53\u001b[0m outs \u001b[39m=\u001b[39m model(g\u001b[39m.\u001b[39mget_deg_list(), g\u001b[39m.\u001b[39mget_edge_index())\n\u001b[0;32m     54\u001b[0m pair \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mget_pair_index()\n",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m, in \u001b[0;36mGraph.get_BC_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m BC_value \u001b[39m=\u001b[39m []\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_list:         \n\u001b[1;32m---> 39\u001b[0m     BC_value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nx\u001b[39m.\u001b[39;49mbetweenness_centrality(g)\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     40\u001b[0m \u001b[39m# for index, value in enumerate(BC_value):\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m#     BC_value[index] = math.log(value+1e-8)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mTensor(BC_value)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\networkx\\classes\\backends.py:145\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m             \u001b[39mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[0;32m    143\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not implemented by \u001b[39m\u001b[39m{\u001b[39;00mplugin_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m             )\n\u001b[1;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 12:4\u001b[0m, in \u001b[0;36margmap_betweenness_centrality_9\u001b[1;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\networkx\\algorithms\\centrality\\betweenness.py:138\u001b[0m, in \u001b[0;36mbetweenness_centrality\u001b[1;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[0;32m    136\u001b[0m         betweenness, _ \u001b[39m=\u001b[39m _accumulate_endpoints(betweenness, S, P, sigma, s)\n\u001b[0;32m    137\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m         betweenness, _ \u001b[39m=\u001b[39m _accumulate_basic(betweenness, S, P, sigma, s)\n\u001b[0;32m    139\u001b[0m \u001b[39m# rescaling\u001b[39;00m\n\u001b[0;32m    140\u001b[0m betweenness \u001b[39m=\u001b[39m _rescale(\n\u001b[0;32m    141\u001b[0m     betweenness,\n\u001b[0;32m    142\u001b[0m     \u001b[39mlen\u001b[39m(G),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     endpoints\u001b[39m=\u001b[39mendpoints,\n\u001b[0;32m    147\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML\\lib\\site-packages\\networkx\\algorithms\\centrality\\betweenness.py:317\u001b[0m, in \u001b[0;36m_accumulate_basic\u001b[1;34m(betweenness, S, P, sigma, s)\u001b[0m\n\u001b[0;32m    315\u001b[0m coeff \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m delta[w]) \u001b[39m/\u001b[39m sigma[w]\n\u001b[0;32m    316\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m P[w]:\n\u001b[1;32m--> 317\u001b[0m     delta[v] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sigma[v] \u001b[39m*\u001b[39m coeff\n\u001b[0;32m    318\u001b[0m \u001b[39mif\u001b[39;00m w \u001b[39m!=\u001b[39m s:\n\u001b[0;32m    319\u001b[0m     betweenness[w] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delta[w]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "min_node = 2000\n",
    "max_node = 3000\n",
    "lr = 0.00005\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node, 16, lr)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "min_node = 2000\n",
    "max_node = 3000\n",
    "lr = 0.00005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "min_node = 2000\n",
    "max_node = 3000\n",
    "lr = 0.00005\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "# 'youtube' test data\n",
    "test_data = []\n",
    "tmp_data = read_test_data('youtube', testing=True)\n",
    "test_data.append(tmp_data)\n",
    "print(\"Test in youtube data:\")\n",
    "test(model, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node：4000-5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "min_node = 4000\n",
    "max_node = 5000\n",
    "\n",
    "model = train_validate(batch_size, min_node, max_node)\n",
    "torch.save(model, './models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "min_node = 4000\n",
    "max_node = 5000\n",
    "model = torch.load('./models/_'+str(batch_size)+'_'+str(min_node)+'_'+str(max_node)+'_model.pt')\n",
    "\n",
    "test_data = []\n",
    "for i in range(30):\n",
    "    tmp_data = read_test_data('synthetic', i)\n",
    "    test_data.append(tmp_data)\n",
    "\n",
    "print(\"Test in synthetic data:\")\n",
    "test(model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin-chia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
